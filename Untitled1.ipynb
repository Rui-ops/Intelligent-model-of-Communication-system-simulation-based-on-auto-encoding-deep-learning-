{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io as sio\n",
    "import math\n",
    "import time\n",
    "import sys\n",
    "import matplotlib.cm as cm\n",
    "import tensorflow as tf\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_print(tmp_var):\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess = tf.Session()\n",
    "    sess.run(init)\n",
    "    print(sess.run(tmp_var))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16 50\n"
     ]
    }
   ],
   "source": [
    "## START: these are the main parameters to set\n",
    "P_in_dBm=-2              # input power in dBm\n",
    "gamma = 1.27            # fiber nonlinearity (set to 0 zero for AWGN or 1.27 for a nonlinear channel)\n",
    "M = 4                   # constellation size\n",
    "\n",
    "# main network parameters and optimization parameters (to be modified for performance improvements)\n",
    "neurons_per_layer = 50 \n",
    "tx_layers = 3\n",
    "rx_layers = 3\n",
    "learning_rate = 0.001\n",
    "iterations = 50000\n",
    "stacks = 20\n",
    "minibatch_size = stacks*M\n",
    "## END: these are the main parameters to set\n",
    "\n",
    "\n",
    "# derived channel parameters\n",
    "channel_uses = 2 # this should be 2: the fiber code will break otherwise\n",
    "assert(channel_uses==2), \"channel uses should be 2\"\n",
    "L=50             # fiber total length\n",
    "K=20               # number of amplification stages (more layers requires more time)\n",
    "\n",
    "P_in=10**(P_in_dBm/10)*0.001\n",
    "Ess=np.sqrt(P_in)\n",
    "SNR_dB=16\n",
    "SNR=10**(SNR_dB/10)\n",
    "sigma2tot=Ess**2/SNR\n",
    "sigma=np.sqrt(sigma2tot/K)\n",
    "\n",
    "\n",
    "#sigma = 3.8505e-4*np.sqrt(2)  # N0= h*v*nsp*(G-1) sigma**2 = BW*N0 \n",
    "#sigma2tot=K*sigma**2\n",
    "#P_in=10**(P_in_dBm/10)*0.001\n",
    "#Ess=np.sqrt(P_in)\n",
    "#SNR=Ess**2/(sigma2tot)\n",
    "#SNR_dB=10*np.log(SNR)\n",
    "print(SNR_dB,L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=====================================================#\n",
    "# Define the components of the computation graph\n",
    "#=====================================================#\n",
    "initializer = tf.contrib.layers.xavier_initializer()\n",
    "# transmitter\n",
    "W_tx = {}\n",
    "b_tx = {}\n",
    "for NN in range(tx_layers):\n",
    "    if NN == 0:\n",
    "        in_neurons = M\n",
    "    else:\n",
    "        in_neurons = neurons_per_layer\n",
    "    if NN == tx_layers - 1:\n",
    "        out_neurons = channel_uses\n",
    "    else:\n",
    "        out_neurons = neurons_per_layer\n",
    "    W_tx[NN] = tf.Variable(initializer([in_neurons, out_neurons]))\n",
    "    b_tx[NN] = tf.Variable(initializer([1, out_neurons]))\n",
    "        \n",
    "# receiver\n",
    "W_rx = {}\n",
    "b_rx = {}\n",
    "for NN in range(rx_layers):\n",
    "    if NN == 0:\n",
    "        in_neurons = channel_uses+1\n",
    "    else:\n",
    "        in_neurons = neurons_per_layer\n",
    "    if NN == rx_layers - 1:\n",
    "        out_neurons = M\n",
    "    else:\n",
    "        out_neurons = neurons_per_layer\n",
    "    W_rx[NN] = tf.Variable(initializer([in_neurons, out_neurons]))\n",
    "    b_rx[NN] = tf.Variable(initializer([1, out_neurons]))  \n",
    "\n",
    "# the encoder\n",
    "def encoder(x):\n",
    "    for NN in range(tx_layers-1):\n",
    "        x = tf.nn.tanh(tf.matmul(x, W_tx[NN]) + b_tx[NN])\n",
    "    x = tf.matmul(x, W_tx[tx_layers-1]) + b_tx[tx_layers-1]\n",
    "    return x\n",
    "\n",
    "# the decoder\n",
    "def decoder(x):\n",
    "    for NN in range(rx_layers-1):\n",
    "        x = tf.nn.tanh(tf.matmul(x, W_rx[NN]) + b_rx[NN])\n",
    "    x = tf.nn.softmax(tf.matmul(x, W_rx[rx_layers-1]) + b_rx[rx_layers-1])\n",
    "    return x\n",
    "\n",
    "# the non-dispersive fiber channel  \n",
    "def fiber_channel(x):\n",
    "    xr=x[:,0]\n",
    "    xi=x[:,1]\n",
    "    for segments in range(1,K+1):               \n",
    "        s=gamma*(xr**2+xi**2)*L/K        \n",
    "        xr=xr*tf.cos(s)-xi*tf.sin(s)\n",
    "        xi=xi*tf.cos(s)+xr*tf.sin(s)\n",
    "        xr=tf.add(xr,tf.random_normal(tf.shape(xr), mean=0.0, stddev=sigma))\n",
    "        xi=tf.add(xi,tf.random_normal(tf.shape(xi), mean=0.0, stddev=sigma)) \n",
    "    z=tf.stack([xr,xi,xr**2+xi**2]) \n",
    "    z=tf.transpose(z) \n",
    "    return z\n",
    "\n",
    "# average transmit power constraint\n",
    "def normalization(x): # E[|x|^2] = Es\n",
    "    return Ess*x / tf.sqrt(2*tf.reduce_mean(tf.square(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {},
   "outputs": [],
   "source": [
    "_map64_ = dict([('0',(-4,-4)),('1',(-4,-3)),('2',(-4,-1)),('3',(-4,-2)), \n",
    "                ('4',(-4,4)),('5',(-4,3)),('6',(-4,1)),('7',(-4,2)),\n",
    "               ('8',(-3,-4)),('9',(-3,-3)),('10',(-3,-1)),('11',(-3,-2)),\n",
    "               ('12',(-3,4)),('13',(-3,3)),('14',(-3,1)),('15',(-3,2)),\n",
    "               ('16',(-1,-4)),('17',(-1,-3)),('18',(-1,-1)),('19',(-1,-2)),\n",
    "               ('20',(-1,4)),('21',(-1,3)),('22',(-1,1)),('23',(-1,2)),\n",
    "               ('24',(-2,-4)),('25',(-2,-3)),('26',(-2,-1)),('27',(-2,-2)),\n",
    "               ('28',(-2,4)),('29',(-2,3)),('30',(-2,1)),('31',(-2,2)),\n",
    "               ('32',(4,-4)),('33',(4,-3)),('34',(4,-1)),('35',(4,-2)),\n",
    "               ('36',(4,4)),('37',(4,3)),('38',(4,1)),('39',(4,2)),\n",
    "               ('40',(3,-4)),('41',(3,-3)),('42',(3,-1)),('43',(3,-2)),\n",
    "               ('44',(3,4)),('45',(3,3)),('46',(3,1)),('47',(3,2)),\n",
    "               ('48',(1,-4)),('49',(1,-3)),('50',(1,-1)),('51',(1,-2)),\n",
    "               ('52',(1,4)),('53',(1,3)),('54',(1,1)),('55',(1,2)),\n",
    "               ('56',(2,-4)),('57',(2,-3)),('58',(2,-1)),('59',(2,-2)),\n",
    "               ('60',(2,4)),('61',(2,3)),('62',(2,1)),('63',(2,2))])\n",
    "_map4_ = dict([('0',(1,1)),('1',(1,-1)),('2',(-1,1)),('3',(-1,-1))])\n",
    "_map16_ = dict([('0',(3,3)),('1',(1,3)),('2',(-3,3)),('3',(-1,3)),        \n",
    "              ('7',(-1,1)),('6',(-3,1)),('5',(1,1)),('4',(3,1)),\n",
    "              ('8',(3,-3)),('9',(1,-3)),('10',(-3,-3)),('11',(-1,-3)),\n",
    "              ('15',(-1,-1)),('14',(-3,-1)),('13',(1,-1)),('12',(3,-1))])\n",
    "#用了Grey Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=====================================================#\n",
    "# build the computation graph\n",
    "#=====================================================#\n",
    "X_tilde = tf.placeholder('float', [minibatch_size, M]) # one-hot vectors\n",
    "enco=tf.placeholder('float', [minibatch_size, 2])\n",
    "\n",
    "#grid coordinates for visulazing decision regions\n",
    "resolution=1000\n",
    "G = tf.placeholder('float', [resolution**2, channel_uses+1])\n",
    "\n",
    "X = normalization(enco) # minibatch_size x channel_uses\n",
    "Y = fiber_channel(X)\n",
    "Z = decoder(Y)\n",
    "D = decoder(G)\n",
    "epsilon = 0.000001\n",
    "loss = -tf.reduce_mean(X_tilde*tf.log(Z+epsilon))\n",
    "MI=(np.log(M)-loss*M)/np.log(2)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "train = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_int_list(start, stop, length):\n",
    "    start, stop = (int(start), int(stop)) if start <= stop else (int(stop), int(start))\n",
    "    length = int(abs(length)) if length else 0\n",
    "    random_list = []\n",
    "    for i in range(length):\n",
    "        random_list.append(random.randint(start, stop))\n",
    "    return random_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  1 : loss =  0.4124269 ; Mutual information [bits] =  -0.3800249\n",
      "iteration  1000 : loss =  0.0018700387 ; Mutual information [bits] =  1.9892085\n",
      "iteration  2000 : loss =  0.0006371564 ; Mutual information [bits] =  1.9963231\n",
      "iteration  3000 : loss =  0.0007933079 ; Mutual information [bits] =  1.995422\n",
      "iteration  4000 : loss =  0.000118813325 ; Mutual information [bits] =  1.9993143\n",
      "iteration  5000 : loss =  5.960318e-05 ; Mutual information [bits] =  1.999656\n",
      "iteration  6000 : loss =  1.2501245e-05 ; Mutual information [bits] =  1.9999279\n",
      "iteration  7000 : loss =  1.9326424e-05 ; Mutual information [bits] =  1.9998885\n",
      "iteration  8000 : loss =  6.169536e-05 ; Mutual information [bits] =  1.9996439\n",
      "iteration  9000 : loss =  1.6802744e-06 ; Mutual information [bits] =  1.9999903\n",
      "iteration  10000 : loss =  1.0413141e-05 ; Mutual information [bits] =  1.9999399\n",
      "iteration  11000 : loss =  9.345453e-06 ; Mutual information [bits] =  1.999946\n",
      "iteration  12000 : loss =  2.7951037e-06 ; Mutual information [bits] =  1.9999838\n",
      "iteration  13000 : loss =  -7.84164e-08 ; Mutual information [bits] =  2.0000005\n",
      "iteration  14000 : loss =  -1.7192198e-07 ; Mutual information [bits] =  2.000001\n",
      "iteration  15000 : loss =  3.1575594e-06 ; Mutual information [bits] =  1.9999818\n",
      "iteration  16000 : loss =  -1.7695109e-07 ; Mutual information [bits] =  2.000001\n",
      "iteration  17000 : loss =  -5.829782e-08 ; Mutual information [bits] =  2.0000002\n",
      "iteration  18000 : loss =  -1.5478489e-07 ; Mutual information [bits] =  2.000001\n",
      "iteration  19000 : loss =  0.0001228236 ; Mutual information [bits] =  1.9992912\n",
      "iteration  20000 : loss =  8.4392994e-08 ; Mutual information [bits] =  1.9999994\n",
      "iteration  21000 : loss =  1.3977479e-08 ; Mutual information [bits] =  2.0\n",
      "iteration  22000 : loss =  2.142005e-06 ; Mutual information [bits] =  1.9999876\n",
      "iteration  23000 : loss =  -2.0395956e-07 ; Mutual information [bits] =  2.0000012\n",
      "iteration  24000 : loss =  5.943042e-08 ; Mutual information [bits] =  1.9999996\n",
      "iteration  25000 : loss =  -2.2444867e-07 ; Mutual information [bits] =  2.0000014\n",
      "iteration  26000 : loss =  -2.3730095e-07 ; Mutual information [bits] =  2.0000014\n",
      "iteration  27000 : loss =  -1.9129352e-07 ; Mutual information [bits] =  2.000001\n",
      "iteration  28000 : loss =  -2.1010631e-07 ; Mutual information [bits] =  2.0000012\n",
      "iteration  29000 : loss =  0.0010058379 ; Mutual information [bits] =  1.9941956\n",
      "iteration  30000 : loss =  -2.1513543e-07 ; Mutual information [bits] =  2.0000012\n",
      "iteration  31000 : loss =  -1.6707854e-07 ; Mutual information [bits] =  2.000001\n",
      "iteration  32000 : loss =  -2.3506577e-07 ; Mutual information [bits] =  2.0000014\n",
      "iteration  33000 : loss =  2.9445455e-06 ; Mutual information [bits] =  1.999983\n",
      "iteration  34000 : loss =  -1.7099045e-07 ; Mutual information [bits] =  2.000001\n",
      "iteration  35000 : loss =  -2.3655589e-07 ; Mutual information [bits] =  2.0000014\n",
      "iteration  36000 : loss =  -1.4845158e-07 ; Mutual information [bits] =  2.000001\n",
      "iteration  37000 : loss =  -2.0544958e-07 ; Mutual information [bits] =  2.0000012\n",
      "iteration  38000 : loss =  -2.3767348e-07 ; Mutual information [bits] =  2.0000014\n",
      "iteration  39000 : loss =  -2.3767348e-07 ; Mutual information [bits] =  2.0000014\n",
      "iteration  40000 : loss =  -1.4174589e-07 ; Mutual information [bits] =  2.000001\n",
      "iteration  41000 : loss =  -2.1476285e-07 ; Mutual information [bits] =  2.0000012\n",
      "iteration  42000 : loss =  2.21367e-06 ; Mutual information [bits] =  1.9999872\n",
      "iteration  43000 : loss =  -2.3730095e-07 ; Mutual information [bits] =  2.0000014\n",
      "iteration  44000 : loss =  9.754481e-06 ; Mutual information [bits] =  1.9999437\n",
      "iteration  45000 : loss =  -2.1364527e-07 ; Mutual information [bits] =  2.0000012\n",
      "iteration  46000 : loss =  -3.8737973e-08 ; Mutual information [bits] =  2.0000002\n",
      "iteration  47000 : loss =  -2.3841854e-07 ; Mutual information [bits] =  2.0000014\n",
      "iteration  48000 : loss =  -2.1643928e-07 ; Mutual information [bits] =  2.0000012\n",
      "iteration  49000 : loss =  -2.3730095e-07 ; Mutual information [bits] =  2.0000014\n",
      "iteration  50000 : loss =  -1.8104876e-07 ; Mutual information [bits] =  2.000001\n",
      "66.26 seconds\n"
     ]
    }
   ],
   "source": [
    "#=====================================================#\n",
    "# parameter training\n",
    "#=====================================================#\n",
    "start_time = time.time()\n",
    "unitmatrix = np.eye(M)  \n",
    "training_set = np.tile(unitmatrix, [stacks, 1])\n",
    "init_op = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init_op)\n",
    "MI_tmp=0\n",
    "totalloss=[]\n",
    "enco_set=[]\n",
    "for i in range(0,stacks):\n",
    "    for j in range(0,M):\n",
    "         enco_set.append(_map4_.get(str(j)))\n",
    "enco_set=np.array(enco_set)\n",
    "for i in range(1, iterations+1):\n",
    "    _, loss_tmp, MI_tmp = sess.run([train, loss, MI], feed_dict={X_tilde: training_set, enco: enco_set})\n",
    "    totalloss=np.append(totalloss, loss_tmp)\n",
    "    if i%1000==0 or i==1:\n",
    "        print('iteration ', i, ': loss = ', loss_tmp, '; Mutual information [bits] = ', MI_tmp)        \n",
    "elapsed = time.time() - start_time\n",
    "print(\"{0:.2f} seconds\".format(elapsed))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BER Calculation\n",
    "test_length=100000\n",
    "X_t = tf.placeholder('float', [test_length, 2]) # one-hot vectors\n",
    "\n",
    "\n",
    "Xt = normalization(X_t) # minibatch_size x channel_uses\n",
    "Yt = fiber_channel(Xt)\n",
    "Zt = decoder(Yt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {},
   "outputs": [],
   "source": [
    "N=test_length\n",
    "test_data=random_int_list(0, M-1, N)\n",
    "test_enco=[]\n",
    "for i in range(0,N):\n",
    "    test_enco.append(_map4_[str(test_data[i])])\n",
    "    \n",
    "[constellation,receive_points,decoded] = sess.run([Xt,Yt,Zt],feed_dict={X_t:test_enco})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BER: 0.00026\n"
     ]
    }
   ],
   "source": [
    "pred_output = np.argmax(decoded,axis=1)\n",
    "no_errors = (pred_output != test_data)\n",
    "no_errors =  no_errors.astype(int).sum()\n",
    "ber = no_errors \n",
    "print ('BER:',ber/N)\n",
    "#print(pred_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
